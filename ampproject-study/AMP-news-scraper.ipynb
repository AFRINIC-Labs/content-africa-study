{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News website scraper\n",
    "\n",
    "Goes through each domain obtained from http://www.abyznewslinks.com/ and extract the URLs containing more than 5 \"-\". Those would normally be press article and the title of the article is usually the same as in the url. We then open the urls one by one to extract the \"amphtml\" tag. We limit the number of AMP pages discovered by domain to 10 as sample (per domain). From the original amp url (found in the source of each AMP-enabled news article, we can build the \"amp_viewer_url\" and the \"AMP Cache url\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "import re\n",
    "\n",
    "#USER_AGENT = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/58: .0.3029.110 Chrome/58.0.3029.110 Safari/537.36'}\n",
    "USER_AGENT = {'User-Agent': 'Mozilla/5.0 (Linux; Android 7.0; SM-G892A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Mobile Safari/537.36'}\n",
    "\n",
    "def getArticleLinks(domain):\n",
    "    url = \"http://\" + domain    \n",
    "    \n",
    "    links = []\n",
    "    series = pd.Series(data=links)\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(url, headers=USER_AGENT)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return series\n",
    "    \n",
    "    response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    c = response.xpath('//a[contains(@href, \"-\")]/@href').extract()\n",
    "    #c = response.xpath('//a/@href').extract()\n",
    "        \n",
    "    my_regex = r\"^https+://.*\" + re.escape(domain) + r\".*\"\n",
    "    #my_regex1 = r\".*\" + re.escape(domain) + r\"/.*\"\n",
    "    #my_regex2 = r\".*\" + re.escape(domain) + r\"/\\d+.html\"\n",
    "    \n",
    "    for link in c:\n",
    "        hyphens = link.count('-')\n",
    "        \n",
    "        #if link has more than 5 hyphens, it is very likely it is a news link\n",
    "        if (hyphens > 5):\n",
    "            #if found most likely it has the http(s) in there too\n",
    "            #if (re.match(my_regex,link, re.IGNORECASE)):\n",
    "                if ('http' in link):\n",
    "                    links.append(link)\n",
    "                else:\n",
    "                    links.append(\"http://\" + domain + '/' + link)\n",
    "        \n",
    "        #if (re.search(my_regex2, link, re.IGNORECASE)):\n",
    "        #    print(link)\n",
    "    series = pd.Series(data=links)\n",
    "    series = series.drop_duplicates(keep='first')\n",
    "    \n",
    "    return series\n",
    "\n",
    "def getAMPUrl(link):\n",
    "    c = None\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(link, headers=USER_AGENT)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return c\n",
    "        \n",
    "    response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    c = response.xpath('//link[contains(@rel, \"amphtml\")]/@href').extract()\n",
    "    \n",
    "    return c\n",
    "\n",
    "def getSampleLinksByCC(df, cc, n):\n",
    "    df_cc = df.loc[df['cc']==cc]\n",
    "    if (len(df_cc) < n):\n",
    "        return df_cc\n",
    "    else:\n",
    "        return df_cc.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = pd.read_csv('data/domain.csv', encoding='latin1')\n",
    "df_domains = df_domains.iloc[:,[0,2,8, 10, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(columns=['domain','cc','geoloc_cc','url','ori_amp_url', 'amp_viewer_url', 'amp_cdn_url'])\n",
    "\n",
    "for index, row in df_domains.iterrows():\n",
    "    links = getArticleLinks(row['domain'])\n",
    "    \n",
    "    if (links is None or links.size < 1):\n",
    "        continue\n",
    "    \n",
    "    count_amp = 0\n",
    "    count_amp_notfound = 0\n",
    "    \n",
    "    for index, url in links.iteritems():\n",
    "        ori_amp_url = getAMPUrl(url)        \n",
    "        if (ori_amp_url is None or len(ori_amp_url) == 0):\n",
    "            \n",
    "            count_amp_notfound = count_amp_notfound + 1\n",
    "            \n",
    "            if (count_amp_notfound > 3):\n",
    "                break\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            count_amp = count_amp + 1 \n",
    "            found = \"\"\n",
    "            m = re.search('https?://(.*)', ori_amp_url[0])\n",
    "            if m:\n",
    "                 found = m.group(1)\n",
    "  \n",
    "            amp_viewer_url = \"https://www.google.com/amp/s/\" + found\n",
    "            amp_cdn_url = \"https://\" + row['domain'].replace('.','-') + \".cdn.ampproject.org/c/s/\" + found\n",
    "                         \n",
    "            #we only want 10 links per domain\n",
    "            if (count_amp > 10):\n",
    "                break\n",
    "            \n",
    "#             print(\"#################    %s    ############\" % row['domain'] )\n",
    "#             print(\"url=\" + url)\n",
    "#             print(\"ori_amp_url=\" + ori_amp_url[0])\n",
    "#             print(\"amp_viewer_url=\" + amp_viewer_url)\n",
    "#             print(\"amp_cdn_url=\" + amp_cdn_url)\n",
    "            \n",
    "            df_links = df_links.append({'domain': row['domain'], \n",
    "                             'cc': row['cc'], \n",
    "                             'geoloc_cc': row['geoloc_cc'], \n",
    "                             'url': url, \n",
    "                             'ori_amp_url': ori_amp_url[0], \n",
    "                             'amp_viewer_url': amp_viewer_url, \n",
    "                             'amp_cdn_url' : amp_cdn_url}, \n",
    "                            ignore_index=True)\n",
    "        \n",
    "#df_links.drop_duplicates(subset=['ori_amp_url','amp_viewer_url','amp_cdn_url'], keep='first', inplace=False)        \n",
    "\n",
    "df_links.to_csv('data/links.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv('data/links.csv',sep='|', encoding='utf-8', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_sample = pd.DataFrame(columns=df_links.columns)\n",
    "\n",
    "for cc in df_links['cc'].drop_duplicates():\n",
    "    df_cc = getSampleLinksByCC(df_links, cc, 15)\n",
    "    df_links_sample = df_links_sample.append(df_cc, ignore_index=True)\n",
    "\n",
    "df_links_sample.to_csv('data/links_sample4.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.groupby('cc')['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_sample.groupby('cc')['url'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                         263chat.com\n",
       "10              africanreporter.co.za\n",
       "15               albertonrecord.co.za\n",
       "18                     alexnews.co.za\n",
       "24                        aminata.com\n",
       "34      bedfordviewedenvalenews.co.za\n",
       "39              benonicitytimes.co.za\n",
       "43                    bereamail.co.za\n",
       "44                 bizwatchnigeria.ng\n",
       "54      blanknewsonline.wordpress.com\n",
       "64           boksburgadvertiser.co.za\n",
       "70                brakpanherald.co.za\n",
       "76                      buzzkenya.com\n",
       "86                    buzznigeria.com\n",
       "96                      citizen.co.za\n",
       "106                    citybuzz.co.za\n",
       "110             comarochronicle.co.za\n",
       "113         completesportsnigeria.com\n",
       "123               crossriverwatch.com\n",
       "133                      dailypost.ng\n",
       "143                  dailystar.com.ng\n",
       "153                       diplomat.so\n",
       "163                         ewn.co.za\n",
       "173      flashpointnews.wordpress.com\n",
       "183              fourwaysreview.co.za\n",
       "189           germistoncitynews.co.za\n",
       "198                    graphic.com.gh\n",
       "208                    guineenews.org\n",
       "218                 highwaymail.co.za\n",
       "223                       iharare.com\n",
       "                    ...              \n",
       "1249                www.techzim.co.zw\n",
       "1259               www.temoignages.re\n",
       "1269         www.thebreakingtimes.com\n",
       "1279                  www.thecable.ng\n",
       "1289             www.thecitizen.co.tz\n",
       "1299         www.theeastafrican.co.ke\n",
       "1319           www.theheartlander.com\n",
       "1329              www.theheraldng.com\n",
       "1339           www.theinfostrides.com\n",
       "1349           www.thenewspaper.co.za\n",
       "1359           www.theperspective.org\n",
       "1361              www.timeslive.co.za\n",
       "1371              www.tsa-algerie.com\n",
       "1381               www.ugandanews.net\n",
       "1382              www.vanguardngr.com\n",
       "1392               www.voandebele.com\n",
       "1395             www.voaportugues.com\n",
       "1405                 www.voashona.com\n",
       "1407              www.voazimbabwe.com\n",
       "1413                   www.yohaig.com\n",
       "1414                    www.youm7.com\n",
       "1424          www.zambianobserver.com\n",
       "1434           www.zambiawatchdog.com\n",
       "1444             www.zimbabwenews.net\n",
       "1445        www.zimbabwesituation.com\n",
       "1455             www.zimbabwestar.com\n",
       "1457                     xalimasn.com\n",
       "1467          zambiabusinesstimes.com\n",
       "1477               zimbabwenews.co.uk\n",
       "1487           zululandobserver.co.za\n",
       "Name: domain, Length: 194, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_links['domain'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
