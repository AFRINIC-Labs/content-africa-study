{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News website scraper\n",
    "\n",
    "Goes through each domain obtained from http://www.abyznewslinks.com/ and extract the URLs containing more than 5 \"-\". Those would normally be press article and the title of the article is usually the same as in the url. We then open the urls one by one to extract the \"amphtml\" tag. We limit the number of AMP pages discovered by domain to 10 as sample (per domain). From the original amp url (found in the source of each AMP-enabled news article, we can build the \"amp_viewer_url\" and the \"AMP Cache url\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "import re\n",
    "\n",
    "#USER_AGENT = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/58: .0.3029.110 Chrome/58.0.3029.110 Safari/537.36'}\n",
    "USER_AGENT = {'User-Agent': 'Mozilla/5.0 (Linux; Android 7.0; SM-G892A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Mobile Safari/537.36'}\n",
    "\n",
    "def getArticleLinks(domain):\n",
    "    url = \"http://\" + domain    \n",
    "    \n",
    "    links = []\n",
    "    series = pd.Series(data=links)\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(url, headers=USER_AGENT)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return series\n",
    "    \n",
    "    response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    c = response.xpath('//a[contains(@href, \"-\")]/@href').extract()\n",
    "    #c = response.xpath('//a/@href').extract()\n",
    "        \n",
    "    my_regex = r\"^https+://.*\" + re.escape(domain) + r\".*\"\n",
    "    #my_regex1 = r\".*\" + re.escape(domain) + r\"/.*\"\n",
    "    #my_regex2 = r\".*\" + re.escape(domain) + r\"/\\d+.html\"\n",
    "    \n",
    "    for link in c:\n",
    "        hyphens = link.count('-')\n",
    "        \n",
    "        #if link has more than 5 hyphens, it is very likely it is a news link\n",
    "        if (hyphens > 5):\n",
    "            #if found most likely it has the http(s) in there too\n",
    "            #if (re.match(my_regex,link, re.IGNORECASE)):\n",
    "                if ('http' in link):\n",
    "                    links.append(link)\n",
    "                else:\n",
    "                    links.append(\"http://\" + domain + '/' + link)\n",
    "        \n",
    "        #if (re.search(my_regex2, link, re.IGNORECASE)):\n",
    "        #    print(link)\n",
    "    series = pd.Series(data=links)\n",
    "    series = series.drop_duplicates(keep='first')\n",
    "    \n",
    "    return series\n",
    "\n",
    "def getAMPUrl(link):\n",
    "    c = None\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(link, headers=USER_AGENT)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return c\n",
    "        \n",
    "    response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    c = response.xpath('//link[contains(@rel, \"amphtml\")]/@href').extract()\n",
    "    \n",
    "    return c\n",
    "\n",
    "def getSampleLinksByCC(df, cc, n):\n",
    "    df_cc = df.loc[df['cc']==cc]\n",
    "    if (len(df_cc) < n):\n",
    "        return df_cc\n",
    "    else:\n",
    "        return df_cc.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = pd.read_csv('data/domain.csv', encoding='latin1')\n",
    "df_domains = df_domains.iloc[:,[0,2,8, 10, 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(columns=['domain','cc','geoloc_cc','url','ori_amp_url', 'amp_viewer_url', 'amp_cdn_url'])\n",
    "\n",
    "for index, row in df_domains.iterrows():\n",
    "    links = getArticleLinks(row['domain'])\n",
    "    \n",
    "    if (links is None or links.size < 1):\n",
    "        continue\n",
    "    \n",
    "    count_amp = 0\n",
    "    count_amp_notfound = 0\n",
    "    \n",
    "    for index, url in links.iteritems():\n",
    "        ori_amp_url = getAMPUrl(url)        \n",
    "        if (ori_amp_url is None or len(ori_amp_url) == 0):\n",
    "            \n",
    "            count_amp_notfound = count_amp_notfound + 1\n",
    "            \n",
    "            if (count_amp_notfound > 3):\n",
    "                break\n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            count_amp = count_amp + 1 \n",
    "            found = \"\"\n",
    "            m = re.search('https?://(.*)', ori_amp_url[0])\n",
    "            if m:\n",
    "                 found = m.group(1)\n",
    "  \n",
    "            amp_viewer_url = \"https://www.google.com/amp/s/\" + found\n",
    "            amp_cdn_url = \"https://\" + row['domain'].replace('.','-') + \".cdn.ampproject.org/c/s/\" + found\n",
    "                         \n",
    "            #we only want 10 links per domain\n",
    "            if (count_amp > 10):\n",
    "                break\n",
    "            \n",
    "#             print(\"#################    %s    ############\" % row['domain'] )\n",
    "#             print(\"url=\" + url)\n",
    "#             print(\"ori_amp_url=\" + ori_amp_url[0])\n",
    "#             print(\"amp_viewer_url=\" + amp_viewer_url)\n",
    "#             print(\"amp_cdn_url=\" + amp_cdn_url)\n",
    "            \n",
    "            df_links = df_links.append({'domain': row['domain'], \n",
    "                             'cc': row['cc'], \n",
    "                             'geoloc_cc': row['geoloc_cc'], \n",
    "                             'url': url, \n",
    "                             'ori_amp_url': ori_amp_url[0], \n",
    "                             'amp_viewer_url': amp_viewer_url, \n",
    "                             'amp_cdn_url' : amp_cdn_url}, \n",
    "                            ignore_index=True)\n",
    "        \n",
    "#df_links.drop_duplicates(subset=['ori_amp_url','amp_viewer_url','amp_cdn_url'], keep='first', inplace=False)        \n",
    "\n",
    "df_links.to_csv('data/links.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.read_csv('data/links.csv',sep='|', encoding='utf-8', keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_sample = pd.DataFrame(columns=df_links.columns)\n",
    "\n",
    "for cc in df_links['cc'].drop_duplicates():\n",
    "    df_cc = getSampleLinksByCC(df_links, cc, 15)\n",
    "    df_links_sample = df_links_sample.append(df_cc, ignore_index=True)\n",
    "\n",
    "df_links_sample.to_csv('data/links_sample4.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.groupby('cc')['domain'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_sample.groupby('cc')['url'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
